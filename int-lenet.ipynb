{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import operator\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Lambda\n",
    "from keras.activations import relu\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load mnist dataset and shape it to be supported by Keras\n",
    "Taken from Keras example on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#set up data for training\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our training model\n",
    "We train our model with a sigmoid activation and floating point weights.  \n",
    "The model trains fast and gets good accuracy (better than that reported in Yann LeCun's paper!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 24, 24, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 30s 503us/step - loss: 2.7715 - acc: 0.6697 - val_loss: 1.9268 - val_acc: 0.8386\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 29s 480us/step - loss: 0.8553 - acc: 0.8992 - val_loss: 0.1924 - val_acc: 0.9490\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 0.1604 - acc: 0.9548 - val_loss: 0.1303 - val_acc: 0.9626\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 28s 459us/step - loss: 0.1107 - acc: 0.9670 - val_loss: 0.1184 - val_acc: 0.9667\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 29s 489us/step - loss: 0.0823 - acc: 0.9746 - val_loss: 0.0894 - val_acc: 0.9754\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 29s 488us/step - loss: 0.0643 - acc: 0.9804 - val_loss: 0.0857 - val_acc: 0.9752\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 28s 474us/step - loss: 0.0529 - acc: 0.9839 - val_loss: 0.0707 - val_acc: 0.9802\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 28s 459us/step - loss: 0.0431 - acc: 0.9861 - val_loss: 0.0742 - val_acc: 0.9795\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 29s 478us/step - loss: 0.0361 - acc: 0.9883 - val_loss: 0.0768 - val_acc: 0.9788\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 28s 459us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0655 - val_acc: 0.9812\n",
      "Test loss: 0.0654917679597\n",
      "Test accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "#model for training with sigmoid activation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                 input_shape=input_shape))\n",
    "model.add(Activation(lambda x: relu(x)))#, max_value=1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(16, (5, 5)))\n",
    "model.add(Activation(lambda x: relu(x)))#, max_value=1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Activation(lambda x: relu(x)))#, max_value=1)))\n",
    "model.add(Dense(84))\n",
    "model.add(Activation(lambda x: relu(x)))#, max_value=1)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#show summary of our model\n",
    "model.summary()\n",
    "\n",
    "#train and test our model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.206849 -0.209384 0.0176228 -0.0864162\n",
      "0.162782 -0.198778 0.0790347 -0.0681564\n",
      "0.190349 -0.198106 0.0420867 -0.0362527\n",
      "0.224855 -0.217727 0.0473939 -0.0331164\n",
      "0.245287 -0.316267 0.0253375 -0.0318747\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    try:\n",
    "        print(np.amax(layer.get_weights()[0]), np.amin(layer.get_weights()[0]),\n",
    "              np.amax(layer.get_weights()[1]), np.amin(layer.get_weights()[1]))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same model, discrete weights, without the sigmoid activation\n",
    "This model will make the same predicitions without the sigmoid activation, and with discrete weights. We ensure we don't cause overflow on the FPGA by dividing by 512 after convolution and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = K.variable(512, name=\"val\")\n",
    "def lfunc(x):\n",
    "    return x/val\n",
    "\n",
    "model_discrete = Sequential()\n",
    "model_discrete.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                 input_shape=input_shape, activation='relu'))\n",
    "model_discrete.add(Lambda(lfunc))\n",
    "#model_discrete.add(Activation(lambda x: relu(x, max_value=65200)))\n",
    "model_discrete.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_discrete.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "model_discrete.add(Lambda(lfunc))\n",
    "#model_discrete.add(Activation(lambda x: relu(x, max_value=65200)))\n",
    "model_discrete.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_discrete.add(Flatten())\n",
    "model_discrete.add(Dense(120, activation='relu'))\n",
    "model_discrete.add(Lambda(lfunc))\n",
    "#model_discrete.add(Activation(lambda x: relu(x, max_value=65200)))\n",
    "model_discrete.add(Dense(84, activation='relu'))\n",
    "model_discrete.add(Lambda(lfunc))\n",
    "#model_discrete.add(Activation(lambda x: relu(x, max_value=65200)))\n",
    "model_discrete.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually evaluate our model on a test set\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "    num_correct = 0\n",
    "    for inp, outp in zip(test_x, test_y):\n",
    "        pred = model.predict(np.reshape(inp, (1, 28, 28, 1)))\n",
    "        max_index, max_value = max(enumerate(pred[0]), key=operator.itemgetter(1))\n",
    "        if int(outp[max_index]) == 1:\n",
    "            num_correct += 1\n",
    "    return num_correct #return # correctly predicted\n",
    "\n",
    "#magic function that applies an operation to every element in a numpy ndarray\n",
    "def mod_ndarray(array, operation):\n",
    "    if array.ndim == 1:\n",
    "        return [operation(x) for x in array]\n",
    "    else:\n",
    "        return [mod_ndarray(x, operation) for x in array]\n",
    "\n",
    "#turns a value from 0 to 1 into uint(8)\n",
    "def mult_255(val):\n",
    "    return round(val*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9804 predicted correctly\n",
      "0.9804 % predicted correctly\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(model.layers)):\n",
    "    try:\n",
    "        model_discrete.layers[i].set_weights([np.asarray(mod_ndarray(model.layers[i].get_weights()[0], mult_255)), np.asarray(mod_ndarray(model.layers[i].get_weights()[1], mult_255))])\n",
    "    except IndexError:\n",
    "        continue #print(\"index error at i =\", i)\n",
    "        \n",
    "num_correct = evaluate_model(model_discrete, x_test, y_test)\n",
    "print(num_correct, \"predicted correctly\")\n",
    "print(num_correct/10000, \"% predicted correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok looks good. Let's save these weights and inputs.\n",
    "Float weights for OpenCL, discrete weights for the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = open(\"inputs.txt\", \"w\")\n",
    "for inp in x_test.flatten():\n",
    "    fh.write(str(inp)+\"\\n\")\n",
    "fh.close()\n",
    "\n",
    "fh = open(\"flat_weights_float.txt\", \"w\")\n",
    "for layer in model.layers:\n",
    "    wgt = layer.get_weights()\n",
    "    if wgt:\n",
    "        weights = wgt[0]\n",
    "        bias = wgt[1]\n",
    "        fh.write(layer.get_config()['name']+\"\\n\")\n",
    "        for s in weights.T.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for weight in weights.T.flatten():\n",
    "            fh.write(str(weight)+\" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for s in bias.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for term in bias.flatten():\n",
    "            fh.write(str(term)+\" \")\n",
    "        fh.write(\"\\n\\n\")\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"flat_weights_discrete.txt\", \"w\")\n",
    "for layer in model_discrete.layers:\n",
    "    wgt = layer.get_weights()\n",
    "    if wgt:\n",
    "        weights = wgt[0]\n",
    "        bias = wgt[1]\n",
    "        fh.write(layer.get_config()['name']+\"\\n\")\n",
    "        for s in weights.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        if 'conv' in layer.get_config()['name']:\n",
    "            for weight in weights.T.flatten():\n",
    "                fh.write(str(weight)+\" \")\n",
    "        else:\n",
    "            for weight in weights.T.flatten():\n",
    "                fh.write\n",
    "            for i in range(0, len(weights)):\n",
    "                for j in range(0, len(weights[0])):\n",
    "                    fh.write(str(weights[i][j]) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for s in bias.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for term in bias.flatten():\n",
    "            fh.write(str(term)+\" \")\n",
    "        fh.write(\"\\n\\n\")\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.   9.   3. ..., -28. -36.  -7.]\n",
      " [ -9.   9.  24. ...,  31.  25. -12.]\n",
      " [ 20.   8. -25. ..., -13.  13.  20.]\n",
      " ..., \n",
      " [ 24.  -1.  15. ..., -22. -21.   7.]\n",
      " [-19.  -5.  36. ..., -19.   6.   6.]\n",
      " [-28.  -4.  -0. ..., -28.  -8.  10.]]\n"
     ]
    }
   ],
   "source": [
    "print(model_discrete.layers[7].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(len(model_discrete.layers[7].get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
