{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import operator\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load mnist dataset and shape it to be supported by Keras\n",
    "Taken from Keras example on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "#set up data for training\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our training model\n",
    "We train our model with a sigmoid activation and floating point weights.  \n",
    "The model trains fast and gets good accuracy (better than that reported in Yann LeCun's paper!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.4212 - acc: 0.8667 - val_loss: 0.0986 - val_acc: 0.9692\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 26s 441us/step - loss: 0.0922 - acc: 0.9722 - val_loss: 0.0679 - val_acc: 0.9770\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 25s 419us/step - loss: 0.0714 - acc: 0.9779 - val_loss: 0.0603 - val_acc: 0.9795\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 23s 388us/step - loss: 0.0602 - acc: 0.9813 - val_loss: 0.0515 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 24s 402us/step - loss: 0.0525 - acc: 0.9842 - val_loss: 0.0474 - val_acc: 0.9843\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 23s 384us/step - loss: 0.0471 - acc: 0.9853 - val_loss: 0.0455 - val_acc: 0.9845\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 24s 393us/step - loss: 0.0426 - acc: 0.9870 - val_loss: 0.0476 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 23s 385us/step - loss: 0.0392 - acc: 0.9881 - val_loss: 0.0406 - val_acc: 0.9866\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 0.0359 - acc: 0.9890 - val_loss: 0.0375 - val_acc: 0.9869\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 23s 388us/step - loss: 0.0325 - acc: 0.9898 - val_loss: 0.0365 - val_acc: 0.9879\n",
      "Test loss: 0.0364587178555\n",
      "Test accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "#model for training with sigmoid activation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#show summary of our model\n",
    "model.summary()\n",
    "\n",
    "#train and test our model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the same model without the sigmoid activation\n",
    "This model will make the same predicitions without the sigmoid activation.  \n",
    "Keras can't evaluate a model unless outputs come between -1 and 1 so we'll have to create our own evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model for predicion without sigmoid activation\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(6, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(120, activation='relu'))\n",
    "model2.add(Dense(84, activation='relu'))\n",
    "model2.add(Dense(num_classes, activation='linear'))\n",
    "\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually evaluate our model on a test set\n",
    "def evaluate_model(model, test_x, test_y):\n",
    "    num_correct = 0\n",
    "    for inp, outp in zip(test_x, test_y):\n",
    "        pred = model2.predict(np.reshape(inp, (1, 28, 28, 1)))\n",
    "        max_index, max_value = max(enumerate(pred[0]), key=operator.itemgetter(1))\n",
    "        if int(outp[max_index]) == 1:\n",
    "            num_correct += 1\n",
    "    return num_correct #return # correctly predicted\n",
    "\n",
    "def copy_model_weights(model1, model2):\n",
    "    for i in range(0, len(model.layers)):\n",
    "        try:\n",
    "            model2.layers[i].set_weights(model1.layers[i].get_weights())\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I prove that we don't need the sigmoid activation.\n",
    "I'll predict with model2 (linear activation) using the same weights as our trained model, and the same test set, and we'll get the exact same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9879 predicted correctly\n",
      "0.9879 precent predicted correctly\n"
     ]
    }
   ],
   "source": [
    "copy_model_weights(model, model2)\n",
    "num_correct = evaluate_model(model2, x_test, y_test)\n",
    "print(num_correct, \"predicted correctly\")\n",
    "print(num_correct/10000, \"precent predicted correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok, great, now let's do this with integer weights.\n",
    "We'll have to create some cool function that allows us to convert our float weights to integer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#magic function that applies an operation to every element in a numpy ndarray\n",
    "def mod_ndarray(array, operation):\n",
    "    if array.ndim == 1:\n",
    "        return [operation(x) for x in array]\n",
    "    else:\n",
    "        return [mod_ndarray(x, operation) for x in array]\n",
    "\n",
    "#our first operation on the weights\n",
    "def mult_256(val):\n",
    "    return int(val*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's convert all of our model2 weights to integers!\n",
    "for i in range(0, len(model.layers)):\n",
    "    try:\n",
    "        model2.layers[i].set_weights([np.asarray(mod_ndarray(model.layers[i].get_weights()[0], mult_256), dtype=int), np.asarray(mod_ndarray(model.layers[i].get_weights()[1], mult_256), dtype=int)])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9879 predicted correctly\n",
      "0.9879 precent predicted correctly\n"
     ]
    }
   ],
   "source": [
    "#using floating point inputs\n",
    "num_correct = evaluate_model(model2, x_test, y_test)\n",
    "print(num_correct, \"predicted correctly\")\n",
    "print(num_correct/10000, \"precent predicted correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Conv2D.get_config of <keras.layers.convolutional.Conv2D object at 0x123069dd8>>\n",
      "[ -2.   4.   3.  -1.   4.   1.  -2.   6.  -4.   0.  -6.   0.  -3.  -1.  14.\n",
      "   8.]\n"
     ]
    }
   ],
   "source": [
    "#this may look too good to be true, but it's real!\n",
    "#here's proof that our weights are in fact integers:\n",
    "#I'll print the bias terms for a 2d\n",
    "print(model2.layers[2].get_config)\n",
    "print(model2.layers[2].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9878 predicted correctly\n",
      "0.9878 precent predicted correctly\n"
     ]
    }
   ],
   "source": [
    "#using integer inputs\n",
    "num_correct = evaluate_model(model2, np.asarray(mod_ndarray(x_test, mult_256), dtype=int), y_test)\n",
    "print(num_correct, \"predicted correctly\")\n",
    "print(num_correct/10000, \"precent predicted correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok looks good. Let's save these flattened weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = open(\"flat_weights.txt\", \"w\")\n",
    "for layer in model2.layers:\n",
    "    wgt = layer.get_weights()\n",
    "    if wgt:\n",
    "        weights = wgt[0]\n",
    "        bias = wgt[1]\n",
    "        fh.write(layer.get_config()['name']+\"\\n\")\n",
    "        for s in weights.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for weight in weights.flatten():\n",
    "            fh.write(str(weight)+\" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for s in bias.shape:\n",
    "            fh.write(str(s) + \" \")\n",
    "        fh.write(\"\\n\")\n",
    "        for term in bias.flatten():\n",
    "            fh.write(str(term)+\" \")\n",
    "        fh.write(\"\\n\\n\")\n",
    "fh.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
